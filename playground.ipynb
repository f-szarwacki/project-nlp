{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQv_2KFChkpp",
        "outputId": "9c0a3cbc-2ff7-4509-97b3-43bc47463539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBMFF4u2Z_VE",
        "outputId": "05c53420-b9b1-4206-fb3b-3b85af74e030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  project-nlp-update-transformers-lib.zip\n",
            "66b432ebc86cad81f4213b0693a339ed01fcb05e\n",
            "   creating: project-nlp-update-transformers-lib/\n",
            "   creating: project-nlp-update-transformers-lib/.github/\n",
            "   creating: project-nlp-update-transformers-lib/.github/workflows/\n",
            "  inflating: project-nlp-update-transformers-lib/.github/workflows/stale.yml  \n",
            "  inflating: project-nlp-update-transformers-lib/.gitignore  \n",
            "  inflating: project-nlp-update-transformers-lib/LICENSE  \n",
            "  inflating: project-nlp-update-transformers-lib/README.md  \n",
            "   creating: project-nlp-update-transformers-lib/SentEval/\n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/.gitignore  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/LICENSE  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/README.md  \n",
            "   creating: project-nlp-update-transformers-lib/SentEval/data/\n",
            "   creating: project-nlp-update-transformers-lib/SentEval/data/downstream/\n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/data/downstream/download_dataset.sh  \n",
            "   creating: project-nlp-update-transformers-lib/SentEval/examples/\n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/bow.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/gensen.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/googleuse.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/infersent.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/models.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/examples/skipthought.py  \n",
            "   creating: project-nlp-update-transformers-lib/SentEval/senteval/\n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/__init__.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/binary.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/engine.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/mrpc.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/probing.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/rank.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/sick.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/snli.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/sst.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/sts.py  \n",
            "   creating: project-nlp-update-transformers-lib/SentEval/senteval/tools/\n",
            " extracting: project-nlp-update-transformers-lib/SentEval/senteval/tools/__init__.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/tools/classifier.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/tools/ranking.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/tools/relatedness.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/tools/validation.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/trec.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/senteval/utils.py  \n",
            "  inflating: project-nlp-update-transformers-lib/SentEval/setup.py  \n",
            "   creating: project-nlp-update-transformers-lib/data/\n",
            "  inflating: project-nlp-update-transformers-lib/data/download_nli.sh  \n",
            "  inflating: project-nlp-update-transformers-lib/data/download_uniprot.sh  \n",
            "  inflating: project-nlp-update-transformers-lib/data/download_wiki.sh  \n",
            "   creating: project-nlp-update-transformers-lib/demo/\n",
            "  inflating: project-nlp-update-transformers-lib/demo/README.md  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/flaskdemo.py  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/gradiodemo.py  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/run_demo_example.sh  \n",
            "   creating: project-nlp-update-transformers-lib/demo/static/\n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/example_query.txt  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/example_sentence.txt  \n",
            "   creating: project-nlp-update-transformers-lib/demo/static/files/\n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/all.js  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/bootstrap.min.js  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/favicon.ico  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/jquery-3.3.1.min.js  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/plogo.png  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/popper.min.js  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/files/style.css  \n",
            "  inflating: project-nlp-update-transformers-lib/demo/static/index.html  \n",
            "  inflating: project-nlp-update-transformers-lib/evaluation.py  \n",
            "   creating: project-nlp-update-transformers-lib/figure/\n",
            "  inflating: project-nlp-update-transformers-lib/figure/demo.gif  \n",
            "  inflating: project-nlp-update-transformers-lib/figure/model.png  \n",
            "  inflating: project-nlp-update-transformers-lib/playground.ipynb  \n",
            "  inflating: project-nlp-update-transformers-lib/requirements.txt  \n",
            "  inflating: project-nlp-update-transformers-lib/run_sup_example.sh  \n",
            "  inflating: project-nlp-update-transformers-lib/run_unsup_example.sh  \n",
            "  inflating: project-nlp-update-transformers-lib/setup.py  \n",
            "   creating: project-nlp-update-transformers-lib/simcse/\n",
            " extracting: project-nlp-update-transformers-lib/simcse/__init__.py  \n",
            "  inflating: project-nlp-update-transformers-lib/simcse/models.py  \n",
            "  inflating: project-nlp-update-transformers-lib/simcse/tool.py  \n",
            "  inflating: project-nlp-update-transformers-lib/simcse/trainers.py  \n",
            "  inflating: project-nlp-update-transformers-lib/simcse_to_huggingface.py  \n",
            "   creating: project-nlp-update-transformers-lib/slides/\n",
            "  inflating: project-nlp-update-transformers-lib/slides/emnlp2021_slides.pdf  \n",
            "  inflating: project-nlp-update-transformers-lib/train.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip project-nlp-update-transformers-lib.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaBZpd20bOiN",
        "outputId": "ab21bbb9-5a99-4070-e9ec-eb9805903994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/project-nlp-update-transformers-lib\n"
          ]
        }
      ],
      "source": [
        "%cd project-nlp-update-transformers-lib/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rpcx-LFfxCw",
        "outputId": "c778d9b8-6db9-400d-e4b1-04165db50785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.30.2 (from -r requirements.txt (line 1))\n",
            "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.10.1)\n",
            "Collecting datasets (from -r requirements.txt (line 3))\n",
            "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.7.2)\n",
            "Collecting gradio (from -r requirements.txt (line 7))\n",
            "  Using cached gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.0.1+cu118)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (67.7.2)\n",
            "Collecting tape_proteins==0.5 (from -r requirements.txt (line 10))\n",
            "  Downloading tape_proteins-0.5-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.2->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements.txt (line 1))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.30.2->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2->-r requirements.txt (line 1)) (4.65.0)\n",
            "Collecting tensorboardX (from tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lmdb (from tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading boto3-1.26.160-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting biopython (from tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 3)) (3.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Collecting aiofiles (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (4.2.2)\n",
            "Collecting fastapi (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (1.10.9)\n",
            "Collecting pydub (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 7)) (2.14.0)\n",
            "Collecting python-multipart (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 8)) (16.0.6)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 7)) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 7))\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 1)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 1)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 7)) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 7))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.160 (from boto3->tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading botocore-1.29.160-py3-none-any.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r requirements.txt (line 7))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 8)) (1.3.0)\n",
            "Collecting protobuf>=4.22.3 (from tensorboardX->tape_proteins==0.5->-r requirements.txt (line 10))\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 7)) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 7))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 7)) (1.1.1)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=6f344e2f986dd08409396a8053e1211598085c8eac1b7678987ad4926a993a60\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: tokenizers, safetensors, pydub, lmdb, ffmpy, xxhash, websockets, uc-micro-py, semantic-version, python-multipart, protobuf, orjson, markdown-it-py, jmespath, h11, dill, biopython, aiofiles, uvicorn, tensorboardX, starlette, multiprocess, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, botocore, transformers, s3transfer, httpx, fastapi, gradio-client, datasets, boto3, tape_proteins, gradio\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 biopython-1.81 boto3-1.26.160 botocore-1.29.160 datasets-2.13.1 dill-0.3.6 fastapi-0.98.0 ffmpy-0.3.0 gradio-3.35.2 gradio-client-0.2.7 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 jmespath-1.0.1 linkify-it-py-2.0.2 lmdb-1.4.1 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 multiprocess-0.70.14 orjson-3.9.1 protobuf-4.23.3 pydub-0.25.1 python-multipart-0.0.6 s3transfer-0.6.1 safetensors-0.3.1 semantic-version-2.10.0 starlette-0.27.0 tape_proteins-0.5 tensorboardX-2.6.1 tokenizers-0.13.3 transformers-4.30.2 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhw6KFmTtzK-",
        "outputId": "5abbb4b2-fb25-480d-ce69-68dd238ace57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGQvno6uqkxE",
        "outputId": "8d5c28a4-1fd5-4f42-cd28-6e12020b107f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "from transformers.utils.import_utils import is_accelerate_available\n",
        "print(is_accelerate_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ6IkE2a_n6w"
      },
      "outputs": [],
      "source": [
        "%cd SentEval/data/downstream/\n",
        "!bash download_dataset.sh\n",
        "%cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUw_I-9ylOQk",
        "outputId": "8423adf4-6c99-45f9-c62f-9ca3e7aa9e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/project-nlp-update-transformers-lib/data\n",
            "--2023-06-26 17:41:58--  https://huggingface.co/datasets/lpiekarski/datasets-for-simcse/resolve/main/uniprot_for_simcse.txt\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.124, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/bc/bc/bcbcbed08034d5ba44d9379d33b0554217896a3a11f0d83bc344698a384335ff/6c67a9c1ed378880f774b130e4c3aaab60a34d1ef2e546f183f485dbfd6a0c74?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27uniprot_for_simcse.txt%3B+filename%3D%22uniprot_for_simcse.txt%22%3B&response-content-type=text%2Fplain&Expires=1688060518&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JjL2JjL2JjYmNiZWQwODAzNGQ1YmE0NGQ5Mzc5ZDMzYjA1NTQyMTc4OTZhM2ExMWYwZDgzYmMzNDQ2OThhMzg0MzM1ZmYvNmM2N2E5YzFlZDM3ODg4MGY3NzRiMTMwZTRjM2FhYWI2MGEzNGQxZWYyZTU0NmYxODNmNDg1ZGJmZDZhMGM3ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgwNjA1MTh9fX1dfQ__&Signature=GaHHXl%7E5r%7EHZZuAWRDfYxe6aZ66WhivSFkFvBj1YQ8JIvkx17t4QX6G7Xg4uGgajRQq7jGzhIuiI%7E1RgZOWnf4UotbpzJmY4oBjTD83hkH0ZUu7M3vgbKJRAxH9W7ZgB9pRjVTTdzrBiOZHCFvcL5wDlpYmlg21npy6iYEAvp89N19ev0ma8LlrGkDf-448vkwClz12ZLBrsnpn7CH7NrMFiU-r33cJ0ZxBrQObnUMoDB1zE3yMSg%7E4b7Tad2onaXQE0fxCusDMJwYXNje1tXoQmiXd3aAnyu694m0Ixy8tEqbtG7MXqYNMr3JDeASfsyVmdV8nDp03iRrtpanDmOQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-06-26 17:41:58--  https://cdn-lfs.huggingface.co/repos/bc/bc/bcbcbed08034d5ba44d9379d33b0554217896a3a11f0d83bc344698a384335ff/6c67a9c1ed378880f774b130e4c3aaab60a34d1ef2e546f183f485dbfd6a0c74?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27uniprot_for_simcse.txt%3B+filename%3D%22uniprot_for_simcse.txt%22%3B&response-content-type=text%2Fplain&Expires=1688060518&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JjL2JjL2JjYmNiZWQwODAzNGQ1YmE0NGQ5Mzc5ZDMzYjA1NTQyMTc4OTZhM2ExMWYwZDgzYmMzNDQ2OThhMzg0MzM1ZmYvNmM2N2E5YzFlZDM3ODg4MGY3NzRiMTMwZTRjM2FhYWI2MGEzNGQxZWYyZTU0NmYxODNmNDg1ZGJmZDZhMGM3ND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgwNjA1MTh9fX1dfQ__&Signature=GaHHXl%7E5r%7EHZZuAWRDfYxe6aZ66WhivSFkFvBj1YQ8JIvkx17t4QX6G7Xg4uGgajRQq7jGzhIuiI%7E1RgZOWnf4UotbpzJmY4oBjTD83hkH0ZUu7M3vgbKJRAxH9W7ZgB9pRjVTTdzrBiOZHCFvcL5wDlpYmlg21npy6iYEAvp89N19ev0ma8LlrGkDf-448vkwClz12ZLBrsnpn7CH7NrMFiU-r33cJ0ZxBrQObnUMoDB1zE3yMSg%7E4b7Tad2onaXQE0fxCusDMJwYXNje1tXoQmiXd3aAnyu694m0Ixy8tEqbtG7MXqYNMr3JDeASfsyVmdV8nDp03iRrtpanDmOQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.249.85.12, 13.249.85.116, 13.249.85.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.249.85.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 207005927 (197M) [text/plain]\n",
            "Saving to: ‘uniprot_for_simcse.txt’\n",
            "\n",
            "uniprot_for_simcse. 100%[===================>] 197.42M  70.9MB/s    in 2.8s    \n",
            "\n",
            "2023-06-26 17:42:01 (70.9 MB/s) - ‘uniprot_for_simcse.txt’ saved [207005927/207005927]\n",
            "\n",
            "/content/project-nlp-update-transformers-lib\n"
          ]
        }
      ],
      "source": [
        "%cd data/\n",
        "!sh download_uniprot.sh\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "looZvvmkhJzu",
        "outputId": "2c861a39-18f3-44aa-82bc-9a59b46255ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-26 19:23:25.166304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "06/26/2023 19:23:26 - INFO - __main__ -   PyTorch: setting up devices\n",
            "06/26/2023 19:23:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: True\n",
            "06/26/2023 19:23:26 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=250,\n",
            "eval_transfer=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=result/unsup-esm2_t6_8M_UR50D/runs/Jun26_19-23-26_0dc4a6c21d56,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=510,\n",
            "metric_for_best_model=stsb_spearman,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=result/unsup-esm2_t6_8M_UR50D,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=512,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=result/unsup-esm2_t6_8M_UR50D,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "06/26/2023 19:23:26 - WARNING - datasets.builder -   Found cached dataset text (/content/project-nlp-update-transformers-lib/data/text/default-84dbcaa000a453c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "100% 1/1 [00:00<00:00, 95.01it/s]\n",
            "[INFO|configuration_utils.py:669] 2023-06-26 19:23:27,015 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t6_8M_UR50D/snapshots/c731040fcd8d73dceaa04b0a8e6329b345b0f5df/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-06-26 19:23:27,016 >> Model config EsmConfig {\n",
            "  \"_name_or_path\": \"facebook/esm2_t6_8M_UR50D\",\n",
            "  \"architectures\": [\n",
            "    \"EsmForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_layer_norm_before\": false,\n",
            "  \"esmfold_config\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 320,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1280,\n",
            "  \"is_folding_model\": false,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"mask_token_id\": 32,\n",
            "  \"max_position_embeddings\": 1026,\n",
            "  \"model_type\": \"esm\",\n",
            "  \"num_attention_heads\": 20,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"rotary\",\n",
            "  \"token_dropout\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_list\": null,\n",
            "  \"vocab_size\": 33\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1823] 2023-06-26 19:23:27,068 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t6_8M_UR50D/snapshots/c731040fcd8d73dceaa04b0a8e6329b345b0f5df/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1823] 2023-06-26 19:23:27,068 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1823] 2023-06-26 19:23:27,068 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t6_8M_UR50D/snapshots/c731040fcd8d73dceaa04b0a8e6329b345b0f5df/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1823] 2023-06-26 19:23:27,068 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t6_8M_UR50D/snapshots/c731040fcd8d73dceaa04b0a8e6329b345b0f5df/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2578] 2023-06-26 19:23:27,069 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--esm2_t6_8M_UR50D/snapshots/c731040fcd8d73dceaa04b0a8e6329b345b0f5df/model.safetensors\n",
            "[WARNING|modeling_utils.py:3285] 2023-06-26 19:23:27,237 >> Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmForCL: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing EsmForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing EsmForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3297] 2023-06-26 19:23:27,237 >> Some weights of EsmForCL were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "06/26/2023 19:23:27 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /content/project-nlp-update-transformers-lib/data/text/default-84dbcaa000a453c0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-0d1dab72696078f7.arrow\n",
            "[INFO|trainer.py:577] 2023-06-26 19:23:30,515 >> max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -   ***** Running training *****\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Num examples = 569516\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Num Epochs = 1\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Instantaneous batch size per device = 512\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Gradient Accumulation steps = 1\n",
            "06/26/2023 19:23:30 - INFO - simcse.trainers -     Total optimization steps = 510\n",
            "{'eval_stsb_spearman': 0.054369192884685565, 'eval_sickr_spearman': 0.2025538236458061, 'eval_avg_sts': 0.12846150826524583, 'epoch': 0.22}\n",
            "{'loss': 0.0919, 'learning_rate': 3.921568627450981e-07, 'epoch': 0.45}\n",
            "{'eval_stsb_spearman': 0.046991143641430824, 'eval_sickr_spearman': 0.2208847677401413, 'eval_avg_sts': 0.13393795569078606, 'epoch': 0.45}\n",
            " 98% 500/510 [07:56<00:09,  1.08it/s][INFO|trainer.py:2926] 2023-06-26 19:31:26,976 >> Saving model checkpoint to result/unsup-esm2_t6_8M_UR50D\n",
            "[INFO|configuration_utils.py:458] 2023-06-26 19:31:26,977 >> Configuration saved in result/unsup-esm2_t6_8M_UR50D/config.json\n",
            "[INFO|modeling_utils.py:1853] 2023-06-26 19:31:27,081 >> Model weights saved in result/unsup-esm2_t6_8M_UR50D/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2194] 2023-06-26 19:31:27,082 >> tokenizer config file saved in result/unsup-esm2_t6_8M_UR50D/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2201] 2023-06-26 19:31:27,082 >> Special tokens file saved in result/unsup-esm2_t6_8M_UR50D/special_tokens_map.json\n",
            "100% 510/510 [08:05<00:00,  1.06s/it]06/26/2023 19:31:36 - INFO - simcse.trainers -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "06/26/2023 19:31:36 - INFO - simcse.trainers -   Loading best model from result/unsup-esm2_t6_8M_UR50D (score: 0.046991143641430824).\n",
            "[INFO|configuration_utils.py:667] 2023-06-26 19:31:36,486 >> loading configuration file result/unsup-esm2_t6_8M_UR50D/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-06-26 19:31:36,487 >> Model config EsmConfig {\n",
            "  \"_name_or_path\": \"facebook/esm2_t6_8M_UR50D\",\n",
            "  \"architectures\": [\n",
            "    \"EsmForCL\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_layer_norm_before\": false,\n",
            "  \"esmfold_config\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 320,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1280,\n",
            "  \"is_folding_model\": false,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"mask_token_id\": 32,\n",
            "  \"max_position_embeddings\": 1026,\n",
            "  \"model_type\": \"esm\",\n",
            "  \"num_attention_heads\": 20,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"rotary\",\n",
            "  \"token_dropout\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.30.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_list\": null,\n",
            "  \"vocab_size\": 33\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2575] 2023-06-26 19:31:36,487 >> loading weights file result/unsup-esm2_t6_8M_UR50D/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:3295] 2023-06-26 19:31:36,599 >> All model checkpoint weights were used when initializing EsmForCL.\n",
            "\n",
            "[INFO|modeling_utils.py:3303] 2023-06-26 19:31:36,599 >> All the weights of EsmForCL were initialized from the model checkpoint at result/unsup-esm2_t6_8M_UR50D.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use EsmForCL for predictions without further training.\n",
            "{'train_runtime': 486.0904, 'train_samples_per_second': 1.049, 'epoch': 0.46}\n",
            "100% 510/510 [08:06<00:00,  1.05it/s]\n",
            "[INFO|trainer.py:2926] 2023-06-26 19:31:36,613 >> Saving model checkpoint to result/unsup-esm2_t6_8M_UR50D\n",
            "[INFO|configuration_utils.py:458] 2023-06-26 19:31:36,614 >> Configuration saved in result/unsup-esm2_t6_8M_UR50D/config.json\n",
            "[INFO|modeling_utils.py:1853] 2023-06-26 19:31:36,694 >> Model weights saved in result/unsup-esm2_t6_8M_UR50D/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2194] 2023-06-26 19:31:36,695 >> tokenizer config file saved in result/unsup-esm2_t6_8M_UR50D/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2201] 2023-06-26 19:31:36,695 >> Special tokens file saved in result/unsup-esm2_t6_8M_UR50D/special_tokens_map.json\n",
            "06/26/2023 19:31:36 - INFO - __main__ -   ***** Train results *****\n",
            "06/26/2023 19:31:36 - INFO - __main__ -     epoch = 0.46\n",
            "06/26/2023 19:31:36 - INFO - __main__ -     train_runtime = 486.0904\n",
            "06/26/2023 19:31:36 - INFO - __main__ -     train_samples_per_second = 1.049\n",
            "06/26/2023 19:31:36 - INFO - __main__ -   *** Evaluate ***\n",
            "06/26/2023 19:31:48 - INFO - root -   Generating sentence embeddings\n",
            "06/26/2023 19:31:52 - INFO - root -   Generated sentence embeddings\n",
            "06/26/2023 19:31:52 - INFO - root -   Training pytorch-MLP-nhid0-rmsprop-bs128 with (inner) 5-fold cross-validation\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m597\u001b[0m in \u001b[92m<module>\u001b[0m        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m594 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m595 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m596 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m597 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m598 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m579\u001b[0m in \u001b[92mmain\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m576 \u001b[0m\u001b[2m│   \u001b[0mresults = {}                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m577 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m training_args.do_eval:                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33m\"\u001b[0m\u001b[33m*** Evaluate ***\u001b[0m\u001b[33m\"\u001b[0m)                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m579 \u001b[2m│   │   \u001b[0mresults = trainer.evaluate(eval_senteval_transfer=\u001b[94mTrue\u001b[0m)        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m580 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m581 \u001b[0m\u001b[2m│   │   \u001b[0moutput_eval_file = os.path.join(training_args.output_dir, \u001b[33m\"\u001b[0m\u001b[33meva\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m582 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m trainer.is_world_process_zero():                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/simcse/\u001b[0m\u001b[1;33mtrainers.py\u001b[0m:\u001b[94m129\u001b[0m in       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mevaluate\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m eval_senteval_transfer \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.args.eval_transfer:          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   │   \u001b[0mtasks = [\u001b[33m'\u001b[0m\u001b[33mSTSBenchmark\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mSICKRelatedness\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mMR\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mCR\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mS\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.model.eval()                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m129 \u001b[2m│   │   \u001b[0mresults = se.eval(tasks)                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m131 \u001b[0m\u001b[2m│   │   \u001b[0mstsb_spearman = results[\u001b[33m'\u001b[0m\u001b[33mSTSBenchmark\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mdev\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mspearman\u001b[0m\u001b[33m'\u001b[0m][\u001b[94m0\u001b[0m]  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   \u001b[0msickr_spearman = results[\u001b[33m'\u001b[0m\u001b[33mSICKRelatedness\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mdev\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mspearman\u001b[0m\u001b[33m'\u001b[0m] \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/\u001b[0m\u001b[1;33mengine.py\u001b[0m:\u001b[94m5\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m9\u001b[0m in \u001b[92meval\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92meval\u001b[0m(\u001b[96mself\u001b[0m, name):                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 57 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# evaluate on evaluation [name], either takes string or list o\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m (\u001b[96misinstance\u001b[0m(name, \u001b[96mlist\u001b[0m)):                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 59 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.results = {x: \u001b[96mself\u001b[0m.eval(x) \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m name}             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.results                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 61 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   │   \u001b[0mtpath = \u001b[96mself\u001b[0m.params.task_path                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/\u001b[0m\u001b[1;33mengine.py\u001b[0m:\u001b[94m5\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m9\u001b[0m in \u001b[92m<dictcomp>\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92meval\u001b[0m(\u001b[96mself\u001b[0m, name):                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 57 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# evaluate on evaluation [name], either takes string or list o\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m (\u001b[96misinstance\u001b[0m(name, \u001b[96mlist\u001b[0m)):                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 59 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.results = {x: \u001b[96mself\u001b[0m.eval(x) \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m name}             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 60 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.results                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 61 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 62 \u001b[0m\u001b[2m│   │   \u001b[0mtpath = \u001b[96mself\u001b[0m.params.task_path                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/\u001b[0m\u001b[1;33mengine.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m27\u001b[0m in \u001b[92meval\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m124 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.params.current_task = name                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.evaluation.do_prepare(\u001b[96mself\u001b[0m.params, \u001b[96mself\u001b[0m.prepare)          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m127 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.results = \u001b[96mself\u001b[0m.evaluation.run(\u001b[96mself\u001b[0m.params, \u001b[96mself\u001b[0m.batcher)  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m129 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.results                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/\u001b[0m\u001b[1;33mbinary.py\u001b[0m:\u001b[94m5\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m7\u001b[0m in \u001b[92mrun\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   │   │     \u001b[0m\u001b[33m'\u001b[0m\u001b[33mclassifier\u001b[0m\u001b[33m'\u001b[0m: params.classifier,                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   │   │   │     \u001b[0m\u001b[33m'\u001b[0m\u001b[33mnhid\u001b[0m\u001b[33m'\u001b[0m: params.nhid, \u001b[33m'\u001b[0m\u001b[33mkfold\u001b[0m\u001b[33m'\u001b[0m: params.kfold}           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   │   \u001b[0mclf = InnerKFoldClassifier(enc_input, np.array(sorted_labels),  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m57 \u001b[2m│   │   \u001b[0mdevacc, testacc = clf.run()                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   │   \u001b[0mlogging.debug(\u001b[33m'\u001b[0m\u001b[33mDev acc : \u001b[0m\u001b[33m{0}\u001b[0m\u001b[33m Test acc : \u001b[0m\u001b[33m{1}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m'\u001b[0m.format(devacc, t \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m {\u001b[33m'\u001b[0m\u001b[33mdevacc\u001b[0m\u001b[33m'\u001b[0m: devacc, \u001b[33m'\u001b[0m\u001b[33macc\u001b[0m\u001b[33m'\u001b[0m: testacc, \u001b[33m'\u001b[0m\u001b[33mndev\u001b[0m\u001b[33m'\u001b[0m: \u001b[96mself\u001b[0m.n_sample \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m'\u001b[0m\u001b[33mntest\u001b[0m\u001b[33m'\u001b[0m: \u001b[96mself\u001b[0m.n_samples}                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/tools/\u001b[0m\u001b[1;33mvalid\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mation.py\u001b[0m:\u001b[94m78\u001b[0m in \u001b[92mrun\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mX_in_train, X_in_test = X_train[inner_train_idx],  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0my_in_train, y_in_test = y_train[inner_train_idx],  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.usepytorch:                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 78 \u001b[2m│   │   │   │   │   │   \u001b[0mclf = MLP(\u001b[96mself\u001b[0m.classifier_config, inputdim=\u001b[96msel\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │     \u001b[0mnclasses=\u001b[96mself\u001b[0m.nclasses, l2reg=reg,   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │     \u001b[0mseed=\u001b[96mself\u001b[0m.seed)                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mclf.fit(X_in_train, y_in_train,                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/tools/\u001b[0m\u001b[1;33mclass\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mifier.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92m__init__\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.loss_fn = nn.CrossEntropyLoss().cuda()                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.loss_fn.size_average = \u001b[94mFalse\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   │   \u001b[0moptim_fn, optim_params = utils.get_optimizer(\u001b[96mself\u001b[0m.optim)       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.optimizer = optim_fn(\u001b[96mself\u001b[0m.model.parameters(), **optim_par \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.optimizer.param_groups[\u001b[94m0\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mweight_decay\u001b[0m\u001b[33m'\u001b[0m] = \u001b[96mself\u001b[0m.l2reg    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/project-nlp-update-transformers-lib/./SentEval/senteval/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m89\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mget_optimizer\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m86 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mException\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mUnknown optimization method: \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m'\u001b[0m % method)   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m87 \u001b[0m\u001b[2m│   \u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m88 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# check that we give good parameters to the optimizer\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m89 \u001b[2m│   \u001b[0mexpected_args = inspect.getargspec(optim_fn.\u001b[92m__init__\u001b[0m)[\u001b[94m0\u001b[0m]            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m90 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94massert\u001b[0m expected_args[:\u001b[94m2\u001b[0m] == [\u001b[33m'\u001b[0m\u001b[33mself\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33mparams\u001b[0m\u001b[33m'\u001b[0m]                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m91 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mall\u001b[0m(k \u001b[95min\u001b[0m expected_args[\u001b[94m2\u001b[0m:] \u001b[94mfor\u001b[0m k \u001b[95min\u001b[0m optim_params.keys()):    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m92 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mException\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mUnexpected parameters: expected \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m, got \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.10/\u001b[0m\u001b[1;33minspect.py\u001b[0m:\u001b[94m1237\u001b[0m in \u001b[92mgetargspec\u001b[0m                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1234 \u001b[0m\u001b[2m│   \u001b[0margs, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1235 \u001b[0m\u001b[2m│   │   \u001b[0mgetfullargspec(func)                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1236 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m kwonlyargs \u001b[95mor\u001b[0m ann:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1237 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mFunction has keyword-only parameters or ann\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1238 \u001b[0m\u001b[2m│   │   │   │   │   │    \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m, use inspect.signature() API which can sup\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1239 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m ArgSpec(args, varargs, varkw, defaults)                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1240 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mValueError: \u001b[0mFunction has keyword-only parameters or annotations, use \n",
            "\u001b[1;35minspect.signature\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m API which can support them\n"
          ]
        }
      ],
      "source": [
        "!python train.py \\\n",
        "--model_name_or_path facebook/esm2_t6_8M_UR50D \\\n",
        "--train_file data/uniprot_for_simcse.txt \\\n",
        "--output_dir result/unsup-esm2_t6_8M_UR50D \\\n",
        "--max_steps 510 \\\n",
        "--per_device_train_batch_size 512 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--max_seq_length  32 \\\n",
        "--metric_for_best_model stsb_spearman \\\n",
        "--load_best_model_at_end \\\n",
        "--pooler_type cls \\\n",
        "--mlp_only_train \\\n",
        "--overwrite_output_dir \\\n",
        "--temp 0.05 \\\n",
        "--do_train \\\n",
        "--fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6TRHjOdubOo",
        "outputId": "de444d81-95ff-4c3f-eefb-462874d1ad99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/project-nlp-update-transformers-lib\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfMlJDHM6P54"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
